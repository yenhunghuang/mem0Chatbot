# å‘é‡è³‡æ–™åº«åˆªé™¤æ©Ÿåˆ¶èˆ‡æ–°èè³‡æ–™ç®¡ç†

**æ ¸å¿ƒå•é¡Œ**:
1. ä¸»æµå‘é‡è³‡æ–™åº«æ˜¯å¦éƒ½æ”¯æ´ç¡¬åˆªé™¤ï¼Ÿ
2. æ¯æ—¥ç¾è‚¡æ–°èå­˜å…¥å‘é‡è³‡æ–™åº«ï¼Œå¦‚ä½•é€éå®šæ™‚åˆªé™¤æ§åˆ¶å®¹é‡ï¼Ÿ

**ç°¡ç­”**: æ˜¯çš„ï¼Œä¸»æµå‘é‡è³‡æ–™åº«éƒ½æ”¯æ´åˆªé™¤åŠŸèƒ½ï¼Œä½†å¯¦ä½œæ©Ÿåˆ¶å„æœ‰ä¸åŒã€‚æ–°èè³‡æ–™çš„å®šæ™‚æ¸…ç†æ˜¯å¯è¡Œçš„ï¼Œä½†æœ‰æ›´å¥½çš„æ›¿ä»£æ–¹æ¡ˆã€‚

---

## ğŸ“‹ ç›®éŒ„

1. [ä¸»æµå‘é‡è³‡æ–™åº«åˆªé™¤æ©Ÿåˆ¶å°æ¯”](#ä¸»æµå‘é‡è³‡æ–™åº«åˆªé™¤æ©Ÿåˆ¶å°æ¯”)
2. [æ–°èè³‡æ–™çš„ç‰¹æ€§èˆ‡æŒ‘æˆ°](#æ–°èè³‡æ–™çš„ç‰¹æ€§èˆ‡æŒ‘æˆ°)
3. [å®¹é‡ç®¡ç†ç­–ç•¥](#å®¹é‡ç®¡ç†ç­–ç•¥)
4. [å¯¦ä½œæ–¹æ¡ˆï¼šæ¯æ—¥ç¾è‚¡æ–°èç³»çµ±](#å¯¦ä½œæ–¹æ¡ˆæ¯æ—¥ç¾è‚¡æ–°èç³»çµ±)
5. [æ•ˆèƒ½èˆ‡æˆæœ¬åˆ†æ](#æ•ˆèƒ½èˆ‡æˆæœ¬åˆ†æ)
6. [æœ€ä½³å¯¦è¸å»ºè­°](#æœ€ä½³å¯¦è¸å»ºè­°)

---

## ğŸ—‚ï¸ ä¸»æµå‘é‡è³‡æ–™åº«åˆªé™¤æ©Ÿåˆ¶å°æ¯”

### 1. ChromaDB

**åˆªé™¤æ©Ÿåˆ¶**: ç¡¬åˆªé™¤ (Hard Delete)

```python
import chromadb

client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection("news")

# åˆªé™¤å–®ä¸€æ–‡æª”
collection.delete(ids=["news_001"])

# æ‰¹æ¬¡åˆªé™¤
collection.delete(ids=["news_001", "news_002", "news_003"])

# æ¢ä»¶åˆªé™¤
collection.delete(
    where={"date": {"$lt": "2025-01-01"}}  # åˆªé™¤ 2025-01-01 ä¹‹å‰çš„æ–°è
)
```

**ç‰¹æ€§**:
- âœ… çœŸæ­£é‡‹æ”¾å„²å­˜ç©ºé–“
- âœ… æ”¯æ´æ¢ä»¶åˆªé™¤ (where clause)
- âœ… é©åˆæœ¬åœ°éƒ¨ç½²
- âŒ ç„¡å…§å»º TTL (Time-To-Live)

---

### 2. Pinecone

**åˆªé™¤æ©Ÿåˆ¶**: ç¡¬åˆªé™¤ (Hard Delete)

```python
import pinecone

pinecone.init(api_key="YOUR_API_KEY")
index = pinecone.Index("news-index")

# åˆªé™¤å–®ä¸€å‘é‡
index.delete(ids=["news_001"])

# æ‰¹æ¬¡åˆªé™¤
index.delete(ids=["news_001", "news_002"])

# æ¢ä»¶åˆªé™¤ï¼ˆéœ€æŒ‡å®š namespaceï¼‰
index.delete(
    filter={"date": {"$lt": "2025-01-01"}},
    namespace="us-stocks"
)

# åˆªé™¤æ•´å€‹ namespace
index.delete(delete_all=True, namespace="old-news")
```

**ç‰¹æ€§**:
- âœ… é›²ç«¯æ‰˜ç®¡ï¼Œè‡ªå‹•ç®¡ç†å„²å­˜
- âœ… æ”¯æ´ namespace éš”é›¢
- âœ… åˆªé™¤å¾Œå³æ™‚ç”Ÿæ•ˆ
- âŒ æŒ‰å‘é‡æ•¸é‡è¨ˆè²»ï¼ˆåˆªé™¤å¯ç¯€çœæˆæœ¬ï¼‰
- âŒ ç„¡å…§å»º TTL

---

### 3. Weaviate

**åˆªé™¤æ©Ÿåˆ¶**: ç¡¬åˆªé™¤ (Hard Delete)

```python
import weaviate

client = weaviate.Client("http://localhost:8080")

# åˆªé™¤å–®ä¸€ç‰©ä»¶
client.data_object.delete(
    uuid="news_001",
    class_name="News"
)

# æ‰¹æ¬¡åˆªé™¤ï¼ˆé€é where filterï¼‰
client.batch.delete_objects(
    class_name="News",
    where={
        "path": ["publishDate"],
        "operator": "LessThan",
        "valueDate": "2025-01-01T00:00:00Z"
    }
)
```

**ç‰¹æ€§**:
- âœ… æ”¯æ´è¤‡é›œæŸ¥è©¢åˆªé™¤
- âœ… å³æ™‚ç”Ÿæ•ˆ
- âœ… å¯é¸é›²ç«¯æˆ–è‡ªå»º
- âŒ ç„¡å…§å»º TTL

---

### 4. Milvus

**åˆªé™¤æ©Ÿåˆ¶**: ç¡¬åˆªé™¤ (Hard Delete)

```python
from pymilvus import Collection

collection = Collection("news")

# åˆªé™¤æŒ‡å®š ID
expr = "id in ['news_001', 'news_002', 'news_003']"
collection.delete(expr)

# æ¢ä»¶åˆªé™¤
expr = "publish_date < '2025-01-01'"
collection.delete(expr)

# éœ€è¦æ‰‹å‹•è§¸ç™¼ compact
collection.compact()
```

**ç‰¹æ€§**:
- âœ… æ”¯æ´ SQL-like è¡¨é”å¼
- âœ… é«˜æ•ˆèƒ½ï¼Œé©åˆå¤§è¦æ¨¡
- âš ï¸ åˆªé™¤å¾Œéœ€æ‰‹å‹• compact æ‰èƒ½é‡‹æ”¾ç©ºé–“
- âŒ ç„¡å…§å»º TTL

---

### 5. Qdrant

**åˆªé™¤æ©Ÿåˆ¶**: ç¡¬åˆªé™¤ (Hard Delete) + **æ”¯æ´ TTL**

```python
from qdrant_client import QdrantClient
from datetime import datetime, timedelta

client = QdrantClient("localhost", port=6333)

# åˆªé™¤å–®ä¸€é»
client.delete(
    collection_name="news",
    points_selector=[1, 2, 3]
)

# æ¢ä»¶åˆªé™¤
client.delete(
    collection_name="news",
    points_selector={
        "filter": {
            "must": [
                {
                    "key": "date",
                    "range": {
                        "lt": "2025-01-01"
                    }
                }
            ]
        }
    }
)

# âœ¨ TTL åŠŸèƒ½ï¼ˆè‡ªå‹•éæœŸï¼‰
client.upsert(
    collection_name="news",
    points=[
        {
            "id": 1,
            "vector": [...],
            "payload": {
                "text": "æ–°èå…§å®¹",
                "ttl": (datetime.now() + timedelta(days=30)).timestamp()
            }
        }
    ]
)
# 30 å¤©å¾Œè‡ªå‹•åˆªé™¤
```

**ç‰¹æ€§**:
- âœ… æ”¯æ´ TTLï¼ˆè‡ªå‹•éæœŸåˆªé™¤ï¼‰
- âœ… é«˜æ•ˆèƒ½éæ¿¾åˆªé™¤
- âœ… é–‹æºä¸”åŠŸèƒ½å®Œæ•´
- âœ… é©åˆæ–°èé¡æ™‚æ•ˆæ€§è³‡æ–™

---

### 6. Elasticsearch (å‘é‡æœç´¢åŠŸèƒ½)

**åˆªé™¤æ©Ÿåˆ¶**: ç¡¬åˆªé™¤ (Hard Delete) + **å…§å»º ILM (Index Lifecycle Management)**

```python
from elasticsearch import Elasticsearch

es = Elasticsearch(["http://localhost:9200"])

# åˆªé™¤å–®ä¸€æ–‡æª”
es.delete(index="news", id="news_001")

# æ‰¹æ¬¡åˆªé™¤
es.delete_by_query(
    index="news",
    body={
        "query": {
            "range": {
                "publish_date": {
                    "lt": "2025-01-01"
                }
            }
        }
    }
)

# âœ¨ ILM ç­–ç•¥ï¼ˆè‡ªå‹•ç®¡ç†ï¼‰
ilm_policy = {
    "policy": {
        "phases": {
            "hot": {
                "actions": {}
            },
            "delete": {
                "min_age": "30d",  # 30 å¤©å¾Œè‡ªå‹•åˆªé™¤
                "actions": {
                    "delete": {}
                }
            }
        }
    }
}

es.ilm.put_lifecycle(policy="news_policy", body=ilm_policy)
```

**ç‰¹æ€§**:
- âœ… å…§å»º ILMï¼ˆè‡ªå‹•ç”Ÿå‘½é€±æœŸç®¡ç†ï¼‰
- âœ… æ”¯æ´è¤‡é›œæŸ¥è©¢åˆªé™¤
- âœ… ä¹…ç¶“è€ƒé©—çš„ä¼æ¥­ç´šæ–¹æ¡ˆ
- âš ï¸ è¼ƒé‡é‡ç´šï¼Œéœ€è¦æ›´å¤šè³‡æº

---

### å°æ¯”ç¸½çµè¡¨

| å‘é‡è³‡æ–™åº« | åˆªé™¤é¡å‹ | æ¢ä»¶åˆªé™¤ | TTL æ”¯æ´ | ç©ºé–“é‡‹æ”¾ | æœ€é©åˆå ´æ™¯ |
|-----------|---------|---------|---------|---------|-----------|
| **ChromaDB** | ç¡¬åˆªé™¤ | âœ… | âŒ | ç«‹å³ | æœ¬åœ°é–‹ç™¼ã€å°è¦æ¨¡ |
| **Pinecone** | ç¡¬åˆªé™¤ | âœ… | âŒ | ç«‹å³ | é›²ç«¯æ‰˜ç®¡ã€ä¼æ¥­ |
| **Weaviate** | ç¡¬åˆªé™¤ | âœ… | âŒ | ç«‹å³ | çŸ¥è­˜åœ–è­œã€è¤‡é›œæŸ¥è©¢ |
| **Milvus** | ç¡¬åˆªé™¤ | âœ… | âŒ | éœ€ compact | å¤§è¦æ¨¡ã€é«˜æ•ˆèƒ½ |
| **Qdrant** | ç¡¬åˆªé™¤ | âœ… | âœ… | ç«‹å³ | **æ–°èã€æ™‚æ•ˆæ€§è³‡æ–™** â­ |
| **Elasticsearch** | ç¡¬åˆªé™¤ | âœ… | âœ… (ILM) | ç«‹å³ | ä¼æ¥­ç´šã€æ··åˆæœç´¢ |

**çµè«–**:
- âœ… **æ‰€æœ‰ä¸»æµå‘é‡è³‡æ–™åº«éƒ½æ”¯æ´ç¡¬åˆªé™¤**
- â­ **Qdrant å’Œ Elasticsearch ç‰¹åˆ¥é©åˆæ–°èé¡è³‡æ–™**ï¼ˆå…§å»º TTL/ILMï¼‰

---

## ğŸ“° æ–°èè³‡æ–™çš„ç‰¹æ€§èˆ‡æŒ‘æˆ°

### æ–°èè³‡æ–™ç‰¹æ€§

```
ç¾è‚¡æ–°èè³‡æ–™ç‰¹é»:
â”œâ”€â”€ æ™‚æ•ˆæ€§å¼·
â”‚   â””â”€â”€ åƒ¹å€¼éš¨æ™‚é–“éæ¸›ï¼ˆä»Šå¤©é‡è¦ï¼Œä¸€å€‹æœˆå¾Œç„¡é—œï¼‰
â”œâ”€â”€ æ•¸é‡é¾å¤§
â”‚   â””â”€â”€ æ¯æ—¥ 10,000+ ç¯‡æ–°èï¼ˆå‡è¨­ï¼‰
â”œâ”€â”€ æ›´æ–°é »ç¹
â”‚   â””â”€â”€ 24/7 æŒçºŒç”¢ç”Ÿ
â””â”€â”€ å„²å­˜æˆæœ¬é«˜
    â””â”€â”€ å‘é‡ + åŸæ–‡ = å¤§é‡ç©ºé–“
```

### å®¹é‡æŒ‘æˆ°

**ç¯„ä¾‹è¨ˆç®—**:

```
å‡è¨­:
- æ¯ç¯‡æ–°è = 1KB æ–‡æœ¬ + 3KB å‘é‡ï¼ˆ768 ç¶­ï¼‰= 4KB
- æ¯æ—¥æ–°èé‡ = 10,000 ç¯‡
- ä¿ç•™å¤©æ•¸ = 30 å¤©

ç¸½å®¹é‡éœ€æ±‚:
= 10,000 ç¯‡/å¤© Ã— 30 å¤© Ã— 4KB
= 300,000 ç¯‡ Ã— 4KB
= 1.2 GB

å¦‚æœä¿ç•™ 1 å¹´:
= 10,000 Ã— 365 Ã— 4KB
= 14.6 GB
```

**å•é¡Œ**:
1. å„²å­˜æˆæœ¬éš¨æ™‚é–“ç·šæ€§å¢é•·
2. èˆŠæ–°èä½”ç”¨ç©ºé–“ä½†å¾ˆå°‘è¢«æŸ¥è©¢
3. æœç´¢æ•ˆèƒ½éš¨è³‡æ–™é‡ä¸‹é™

---

## ğŸ¯ å®¹é‡ç®¡ç†ç­–ç•¥

### ç­–ç•¥ 1: å®šæ™‚åˆªé™¤ï¼ˆæ‚¨æåˆ°çš„æ–¹æ¡ˆï¼‰

**å¯¦ä½œ**: æ¯æ—¥åŸ·è¡Œæ¸…ç†ä»»å‹™

```python
from datetime import datetime, timedelta
import chromadb

def cleanup_old_news(days_to_keep: int = 30):
    """åˆªé™¤è¶…é N å¤©çš„æ–°è"""

    client = chromadb.PersistentClient(path="./news_db")
    collection = client.get_collection("us_stock_news")

    # è¨ˆç®—æˆªæ­¢æ—¥æœŸ
    cutoff_date = (datetime.now() - timedelta(days=days_to_keep)).isoformat()

    # æ¢ä»¶åˆªé™¤
    collection.delete(
        where={
            "publish_date": {"$lt": cutoff_date}
        }
    )

    print(f"å·²åˆªé™¤ {cutoff_date} ä¹‹å‰çš„æ–°è")

# ä½¿ç”¨ cron job æ¯æ—¥åŸ·è¡Œ
# 0 2 * * * python cleanup_news.py  # æ¯å¤©å‡Œæ™¨ 2 é»åŸ·è¡Œ
```

**å„ªé»**:
- âœ… ç°¡å–®ç›´æ¥
- âœ… å®¹é‡å¯æ§
- âœ… é©ç”¨æ–¼æ‰€æœ‰å‘é‡è³‡æ–™åº«

**ç¼ºé»**:
- âŒ éœ€è¦é¡å¤–ç¶­è­·å®šæ™‚ä»»å‹™
- âŒ åˆªé™¤æ“ä½œå¯èƒ½å½±éŸ¿æ•ˆèƒ½
- âŒ ç¡¬æ€§åˆªé™¤ï¼Œç„¡æ³•å½ˆæ€§èª¿æ•´

---

### ç­–ç•¥ 2: ä½¿ç”¨ TTL è‡ªå‹•éæœŸï¼ˆæ¨è–¦ â­ï¼‰

**å¯¦ä½œ**: ä½¿ç”¨ Qdrant çš„ TTL åŠŸèƒ½

```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from datetime import datetime, timedelta

client = QdrantClient("localhost", port=6333)

# å»ºç«‹é›†åˆ
client.create_collection(
    collection_name="us_stock_news",
    vectors_config=VectorParams(size=768, distance=Distance.COSINE)
)

def add_news_with_ttl(news_id: str, text: str, embedding: list, days: int = 30):
    """æ–°å¢æ–°èä¸¦è¨­å®š TTL"""

    # è¨ˆç®—éæœŸæ™‚é–“æˆ³
    expire_at = datetime.now() + timedelta(days=days)

    client.upsert(
        collection_name="us_stock_news",
        points=[
            PointStruct(
                id=news_id,
                vector=embedding,
                payload={
                    "text": text,
                    "publish_date": datetime.now().isoformat(),
                    "expire_at": expire_at.timestamp()  # TTL æ¬„ä½
                }
            )
        ]
    )

# Qdrant æœƒè‡ªå‹•åˆªé™¤éæœŸè³‡æ–™ï¼Œç„¡éœ€æ‰‹å‹•å¹²é 
```

**å„ªé»**:
- âœ… è‡ªå‹•åŒ–ï¼Œç„¡éœ€ç¶­è­·å®šæ™‚ä»»å‹™
- âœ… ç²¾ç¢ºåˆ°ç§’ç´šçš„éæœŸæ§åˆ¶
- âœ… ä¸å½±éŸ¿ç·šä¸Šæœå‹™æ•ˆèƒ½

**ç¼ºé»**:
- âš ï¸ éœ€ä½¿ç”¨æ”¯æ´ TTL çš„è³‡æ–™åº«ï¼ˆQdrant, Elasticsearchï¼‰

---

### ç­–ç•¥ 3: æ»¾å‹•ç´¢å¼•ï¼ˆRolling Indexï¼‰

**å¯¦ä½œ**: æŒ‰æ—¥æœŸå»ºç«‹ç¨ç«‹é›†åˆ/ç´¢å¼•

```python
from datetime import datetime
import chromadb

def get_daily_collection(date: datetime):
    """ç²å–ç•¶æ—¥çš„é›†åˆ"""

    client = chromadb.PersistentClient(path="./news_db")
    collection_name = f"news_{date.strftime('%Y%m%d')}"

    return client.get_or_create_collection(collection_name)

def add_news(text: str, embedding: list):
    """æ–°å¢æ–°èåˆ°ç•¶æ—¥é›†åˆ"""

    today = datetime.now()
    collection = get_daily_collection(today)

    collection.add(
        ids=[f"news_{today.timestamp()}"],
        embeddings=[embedding],
        documents=[text]
    )

def search_recent_news(query_embedding: list, days: int = 30):
    """æœç´¢æœ€è¿‘ N å¤©çš„æ–°è"""

    results = []
    client = chromadb.PersistentClient(path="./news_db")

    # éæ­·æœ€è¿‘ N å¤©çš„é›†åˆ
    for i in range(days):
        date = datetime.now() - timedelta(days=i)
        collection_name = f"news_{date.strftime('%Y%m%d')}"

        try:
            collection = client.get_collection(collection_name)
            day_results = collection.query(
                query_embeddings=[query_embedding],
                n_results=10
            )
            results.extend(day_results['documents'][0])
        except:
            continue  # é›†åˆä¸å­˜åœ¨ï¼Œè·³é

    return results

def cleanup_old_collections(days_to_keep: int = 30):
    """åˆªé™¤èˆŠçš„é›†åˆï¼ˆæ•´å€‹é›†åˆï¼‰"""

    client = chromadb.PersistentClient(path="./news_db")
    cutoff_date = datetime.now() - timedelta(days=days_to_keep)

    # åˆªé™¤èˆŠé›†åˆ
    old_collection = f"news_{cutoff_date.strftime('%Y%m%d')}"
    try:
        client.delete_collection(old_collection)
        print(f"å·²åˆªé™¤é›†åˆ: {old_collection}")
    except:
        pass
```

**å„ªé»**:
- âœ… åˆªé™¤æ¥µå¿«ï¼ˆç›´æ¥åˆªé™¤æ•´å€‹é›†åˆï¼‰
- âœ… éš”é›¢æ€§å¥½ï¼ˆä¸åŒæ—¥æœŸäº’ä¸å½±éŸ¿ï¼‰
- âœ… ä¾¿æ–¼å‚™ä»½å’Œæ¢å¾©

**ç¼ºé»**:
- âŒ è·¨æ—¥æœŸæœç´¢è¤‡é›œ
- âŒ ç®¡ç†å¤šå€‹é›†åˆè¼ƒç¹ç‘£

---

### ç­–ç•¥ 4: å†·ç†±è³‡æ–™åˆ†é›¢

**å¯¦ä½œ**: è¿‘æœŸæ–°è (ç†±è³‡æ–™) vs æ­·å²æ–°è (å†·è³‡æ–™)

```python
import chromadb
from datetime import datetime, timedelta

client = chromadb.PersistentClient(path="./news_db")

# ç†±è³‡æ–™ï¼šæœ€è¿‘ 7 å¤©ï¼Œå¿«é€Ÿå­˜å–
hot_collection = client.get_or_create_collection("news_hot")

# æº«è³‡æ–™ï¼š8-30 å¤©ï¼Œæ™®é€šå­˜å–
warm_collection = client.get_or_create_collection("news_warm")

# å†·è³‡æ–™ï¼š31-90 å¤©ï¼Œæ­¸æª”ï¼ˆé™ä½ç²¾åº¦æˆ–å£“ç¸®ï¼‰
cold_collection = client.get_or_create_collection("news_cold")

def add_news(text: str, embedding: list):
    """æ–°å¢åˆ°ç†±è³‡æ–™"""
    hot_collection.add(
        ids=[f"news_{datetime.now().timestamp()}"],
        embeddings=[embedding],
        documents=[text],
        metadatas=[{"date": datetime.now().isoformat()}]
    )

def migrate_to_warm():
    """å°‡ 7 å¤©å‰çš„è³‡æ–™å¾ç†±è³‡æ–™é·ç§»åˆ°æº«è³‡æ–™"""

    cutoff = (datetime.now() - timedelta(days=7)).isoformat()

    # æŸ¥è©¢èˆŠè³‡æ–™
    old_data = hot_collection.get(
        where={"date": {"$lt": cutoff}}
    )

    if old_data['ids']:
        # è¤‡è£½åˆ°æº«è³‡æ–™
        warm_collection.add(
            ids=old_data['ids'],
            embeddings=old_data['embeddings'],
            documents=old_data['documents'],
            metadatas=old_data['metadatas']
        )

        # å¾ç†±è³‡æ–™åˆªé™¤
        hot_collection.delete(ids=old_data['ids'])

def search_news(query_embedding: list):
    """æ™ºèƒ½æœç´¢ï¼šå„ªå…ˆæœç´¢ç†±è³‡æ–™"""

    # å…ˆæœç´¢ç†±è³‡æ–™
    hot_results = hot_collection.query(
        query_embeddings=[query_embedding],
        n_results=20
    )

    # å¦‚æœçµæœä¸è¶³ï¼Œå†æœç´¢æº«è³‡æ–™
    if len(hot_results['documents'][0]) < 10:
        warm_results = warm_collection.query(
            query_embeddings=[query_embedding],
            n_results=10
        )
        # åˆä½µçµæœ
        ...

    return hot_results
```

**å„ªé»**:
- âœ… ç†±è³‡æ–™é«˜æ•ˆèƒ½
- âœ… å†·è³‡æ–™ä½æˆæœ¬
- âœ… å½ˆæ€§ç®¡ç†

**ç¼ºé»**:
- âŒ å¯¦ä½œè¤‡é›œ
- âŒ éœ€è¦è³‡æ–™é·ç§»é‚è¼¯

---

### ç­–ç•¥å°æ¯”

| ç­–ç•¥ | å¯¦ä½œé›£åº¦ | ç¶­è­·æˆæœ¬ | æ•ˆèƒ½å½±éŸ¿ | é©ç”¨å ´æ™¯ |
|------|---------|---------|---------|---------|
| **å®šæ™‚åˆªé™¤** | â­ ç°¡å–® | ä¸­ç­‰ | ä½ | å°è¦æ¨¡ã€ç°¡å–®éœ€æ±‚ |
| **TTL è‡ªå‹•éæœŸ** | â­â­ ç°¡å–® | æ¥µä½ | ç„¡ | **æ¨è–¦é¦–é¸** â­ |
| **æ»¾å‹•ç´¢å¼•** | â­â­â­ ä¸­ç­‰ | ä¸­ç­‰ | ä½ | éœ€è¦æŒ‰æ—¥æœŸéš”é›¢ |
| **å†·ç†±åˆ†é›¢** | â­â­â­â­ è¤‡é›œ | é«˜ | ä½ | å¤§è¦æ¨¡ã€å¤šå±¤æ¬¡éœ€æ±‚ |

---

## ğŸ’» å¯¦ä½œæ–¹æ¡ˆï¼šæ¯æ—¥ç¾è‚¡æ–°èç³»çµ±

### æ–¹æ¡ˆ A: ChromaDB + å®šæ™‚æ¸…ç†ï¼ˆç°¡å–®æ–¹æ¡ˆï¼‰

**é©ç”¨**: å°è¦æ¨¡ã€å¿«é€Ÿå•Ÿå‹•

```python
# news_manager.py
import chromadb
from datetime import datetime, timedelta
import schedule
import time

class NewsVectorDB:
    def __init__(self, db_path: str = "./news_db", days_to_keep: int = 30):
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_or_create_collection(
            name="us_stock_news",
            metadata={"description": "ç¾è‚¡æ–°èå‘é‡è³‡æ–™åº«"}
        )
        self.days_to_keep = days_to_keep

    def add_news(self, news_id: str, title: str, content: str, embedding: list):
        """æ–°å¢æ–°è"""
        self.collection.add(
            ids=[news_id],
            embeddings=[embedding],
            documents=[content],
            metadatas=[{
                "title": title,
                "publish_date": datetime.now().isoformat(),
                "source": "yahoo_finance"
            }]
        )

    def search_news(self, query_embedding: list, top_k: int = 10):
        """æœç´¢ç›¸é—œæ–°è"""
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            include=["documents", "metadatas", "distances"]
        )
        return results

    def cleanup_old_news(self):
        """åˆªé™¤èˆŠæ–°è"""
        cutoff_date = (datetime.now() - timedelta(days=self.days_to_keep)).isoformat()

        # ç²å–èˆŠæ–°è
        all_news = self.collection.get(include=["metadatas"])
        old_ids = [
            id for id, meta in zip(all_news['ids'], all_news['metadatas'])
            if meta.get('publish_date', '') < cutoff_date
        ]

        if old_ids:
            self.collection.delete(ids=old_ids)
            print(f"[{datetime.now()}] åˆªé™¤ {len(old_ids)} æ¢èˆŠæ–°è")

    def get_stats(self):
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        count = self.collection.count()
        return {
            "total_news": count,
            "days_kept": self.days_to_keep,
            "estimated_size_mb": count * 4 / 1024  # ä¼°ç®—
        }

# ä½¿ç”¨ç¯„ä¾‹
db = NewsVectorDB(days_to_keep=30)

# æ¯æ—¥å‡Œæ™¨ 2 é»åŸ·è¡Œæ¸…ç†
schedule.every().day.at("02:00").do(db.cleanup_old_news)

while True:
    schedule.run_pending()
    time.sleep(60)
```

**éƒ¨ç½²**:
```bash
# ä½¿ç”¨ systemd æˆ– supervisor é‹è¡Œ
# æˆ–ä½¿ç”¨ cron job
0 2 * * * cd /path/to/project && python news_manager.py
```

---

### æ–¹æ¡ˆ B: Qdrant + TTLï¼ˆæ¨è–¦æ–¹æ¡ˆ â­ï¼‰

**é©ç”¨**: ç”Ÿç”¢ç’°å¢ƒã€è‡ªå‹•åŒ–

```python
# news_manager_qdrant.py
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from datetime import datetime, timedelta

class NewsVectorDBWithTTL:
    def __init__(self, host: str = "localhost", port: int = 6333):
        self.client = QdrantClient(host=host, port=port)
        self.collection_name = "us_stock_news"

        # å»ºç«‹é›†åˆ
        try:
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=768,  # Google Embeddings ç¶­åº¦
                    distance=Distance.COSINE
                )
            )
        except:
            pass  # é›†åˆå·²å­˜åœ¨

    def add_news(
        self,
        news_id: str,
        title: str,
        content: str,
        embedding: list,
        ttl_days: int = 30
    ):
        """æ–°å¢æ–°èä¸¦è¨­å®š TTL"""

        # è¨ˆç®—éæœŸæ™‚é–“
        expire_at = datetime.now() + timedelta(days=ttl_days)

        self.client.upsert(
            collection_name=self.collection_name,
            points=[
                PointStruct(
                    id=news_id,
                    vector=embedding,
                    payload={
                        "title": title,
                        "content": content,
                        "publish_date": datetime.now().isoformat(),
                        "expire_at": expire_at.timestamp(),  # TTL
                        "source": "yahoo_finance"
                    }
                )
            ]
        )

    def search_news(self, query_embedding: list, top_k: int = 10):
        """æœç´¢æ–°èï¼ˆè‡ªå‹•éæ¿¾éæœŸè³‡æ–™ï¼‰"""

        now = datetime.now().timestamp()

        results = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_embedding,
            limit=top_k,
            query_filter={
                "must": [
                    {
                        "key": "expire_at",
                        "range": {"gt": now}  # åªæœç´¢æœªéæœŸçš„
                    }
                ]
            }
        )

        return results

    def get_stats(self):
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        info = self.client.get_collection(self.collection_name)
        return {
            "total_vectors": info.vectors_count,
            "points_count": info.points_count
        }

# ä½¿ç”¨ç¯„ä¾‹
db = NewsVectorDBWithTTL()

# æ–°å¢æ–°èï¼ˆ30 å¤©å¾Œè‡ªå‹•éæœŸï¼‰
db.add_news(
    news_id="news_20251105_001",
    title="Apple è‚¡åƒ¹å‰µæ–°é«˜",
    content="è˜‹æœå…¬å¸ä»Šæ—¥è‚¡åƒ¹...",
    embedding=[0.1, 0.2, ...],  # 768 ç¶­å‘é‡
    ttl_days=30  # 30 å¤©å¾Œè‡ªå‹•åˆªé™¤
)

# æœç´¢ï¼ˆç„¡éœ€æ‰‹å‹•æ¸…ç†ï¼Œè‡ªå‹•éæ¿¾éæœŸè³‡æ–™ï¼‰
results = db.search_news(query_embedding=[...], top_k=10)
```

**å„ªé»**:
- âœ… é›¶ç¶­è­·ï¼ˆè‡ªå‹•éæœŸï¼‰
- âœ… ç„¡éœ€å®šæ™‚ä»»å‹™
- âœ… æ•ˆèƒ½æ›´å¥½

---

### æ–¹æ¡ˆ C: æ»¾å‹•ç´¢å¼•ï¼ˆä¼æ¥­ç´šæ–¹æ¡ˆï¼‰

**é©ç”¨**: å¤§è¦æ¨¡ã€éœ€è¦æŒ‰æ—¥æœŸéš”é›¢

```python
# news_manager_rolling.py
import chromadb
from datetime import datetime, timedelta

class RollingNewsVectorDB:
    def __init__(self, db_path: str = "./news_db"):
        self.client = chromadb.PersistentClient(path=db_path)

    def _get_date_collection(self, date: datetime):
        """ç²å–æŒ‡å®šæ—¥æœŸçš„é›†åˆ"""
        collection_name = f"news_{date.strftime('%Y%m%d')}"
        return self.client.get_or_create_collection(collection_name)

    def add_news(self, news_id: str, content: str, embedding: list):
        """æ–°å¢æ–°èåˆ°ç•¶æ—¥é›†åˆ"""
        today = datetime.now()
        collection = self._get_date_collection(today)

        collection.add(
            ids=[news_id],
            embeddings=[embedding],
            documents=[content],
            metadatas=[{"date": today.isoformat()}]
        )

    def search_news(self, query_embedding: list, days: int = 7, top_k: int = 10):
        """æœç´¢æœ€è¿‘ N å¤©çš„æ–°è"""
        all_results = []

        for i in range(days):
            date = datetime.now() - timedelta(days=i)
            collection_name = f"news_{date.strftime('%Y%m%d')}"

            try:
                collection = self.client.get_collection(collection_name)
                results = collection.query(
                    query_embeddings=[query_embedding],
                    n_results=top_k
                )
                all_results.extend(results['documents'][0])
            except:
                continue

        return all_results[:top_k]

    def cleanup_old_collections(self, days_to_keep: int = 30):
        """åˆªé™¤èˆŠé›†åˆ"""
        deleted_count = 0

        # åˆªé™¤è¶…é days_to_keep å¤©çš„é›†åˆ
        for i in range(days_to_keep, days_to_keep + 7):  # å¤šæª¢æŸ¥ 7 å¤©
            date = datetime.now() - timedelta(days=i)
            collection_name = f"news_{date.strftime('%Y%m%d')}"

            try:
                self.client.delete_collection(collection_name)
                deleted_count += 1
                print(f"åˆªé™¤é›†åˆ: {collection_name}")
            except:
                pass

        return deleted_count

# å®šæ™‚æ¸…ç†ï¼ˆæ¯æ—¥åŸ·è¡Œï¼‰
db = RollingNewsVectorDB()
db.cleanup_old_collections(days_to_keep=30)
```

---

## ğŸ“Š æ•ˆèƒ½èˆ‡æˆæœ¬åˆ†æ

### å®¹é‡ä¼°ç®—

```python
def calculate_storage(
    daily_news_count: int,
    days_to_keep: int,
    vector_dim: int = 768,
    text_avg_chars: int = 1000
):
    """è¨ˆç®—å„²å­˜éœ€æ±‚"""

    # å‘é‡å¤§å°ï¼ˆfloat32 = 4 bytesï¼‰
    vector_size_kb = vector_dim * 4 / 1024

    # æ–‡æœ¬å¤§å°ï¼ˆUTF-8ï¼Œç´„ 3 bytes per charï¼‰
    text_size_kb = text_avg_chars * 3 / 1024

    # å–®ç¯‡æ–°èç¸½å¤§å°
    per_news_kb = vector_size_kb + text_size_kb

    # ç¸½å®¹é‡
    total_news = daily_news_count * days_to_keep
    total_size_mb = total_news * per_news_kb / 1024

    return {
        "total_news": total_news,
        "total_size_mb": round(total_size_mb, 2),
        "total_size_gb": round(total_size_mb / 1024, 2),
        "per_news_kb": round(per_news_kb, 2)
    }

# ç¯„ä¾‹è¨ˆç®—
result = calculate_storage(
    daily_news_count=10000,
    days_to_keep=30
)

print(result)
# è¼¸å‡º:
# {
#   "total_news": 300000,
#   "total_size_mb": 1171.88,
#   "total_size_gb": 1.14,
#   "per_news_kb": 4.00
# }
```

### ä¸åŒä¿ç•™æœŸé™çš„å®¹é‡å°æ¯”

| ä¿ç•™å¤©æ•¸ | æ–°èç¸½æ•¸ (10k/å¤©) | å„²å­˜å®¹é‡ | æœç´¢æ•ˆèƒ½ |
|---------|------------------|---------|---------|
| 7 å¤© | 70,000 | 273 MB | æ¥µå¿« âš¡âš¡âš¡ |
| 30 å¤© | 300,000 | 1.17 GB | å¿« âš¡âš¡ |
| 90 å¤© | 900,000 | 3.52 GB | ä¸­ç­‰ âš¡ |
| 365 å¤© | 3,650,000 | 14.26 GB | æ…¢ ğŸŒ |

**å»ºè­°**: ä¿ç•™ **30 å¤©** æ˜¯å¹³è¡¡é»

---

### é›²ç«¯æœå‹™æˆæœ¬ä¼°ç®— (Pinecone ç‚ºä¾‹)

```
Pinecone å®šåƒ¹ (2025):
- Starter Plan: å…è²» (100K å‘é‡, 1 Pod)
- Standard Plan: $70/æœˆ (100K å‘é‡, 1 Pod)
- é¡å¤–å‘é‡: $0.096/1000 å‘é‡/æœˆ

30 å¤©æ–°èæˆæœ¬:
= 300,000 å‘é‡ Ã— $0.096 / 1000
= $28.8/æœˆ

ä¸€å¹´æˆæœ¬:
= 3,650,000 å‘é‡ Ã— $0.096 / 1000
= $350.4/æœˆ

ä½¿ç”¨å®šæ™‚åˆªé™¤ä¿æŒ 30 å¤©:
= $28.8/æœˆ (å›ºå®š)

ç¯€çœæˆæœ¬:
= $350.4 - $28.8
= $321.6/æœˆ âœ“
```

**çµè«–**: å®šæ™‚åˆªé™¤å¯å¤§å¹…é™ä½é›²ç«¯æˆæœ¬

---

## âœ… æœ€ä½³å¯¦è¸å»ºè­°

### 1. æ ¹æ“šè¦æ¨¡é¸æ“‡æ–¹æ¡ˆ

```
å°è¦æ¨¡ (< 100K å‘é‡):
â†’ ChromaDB + å®šæ™‚åˆªé™¤
â†’ ç°¡å–®ã€ä½æˆæœ¬ã€æ˜“ç¶­è­·

ä¸­è¦æ¨¡ (100K - 1M å‘é‡):
â†’ Qdrant + TTL
â†’ è‡ªå‹•åŒ–ã€é«˜æ•ˆèƒ½ã€æ¨è–¦ â­

å¤§è¦æ¨¡ (> 1M å‘é‡):
â†’ Elasticsearch + ILM æˆ– æ»¾å‹•ç´¢å¼•
â†’ ä¼æ¥­ç´šã€å¯æ“´å±•
```

### 2. è¨­å®šåˆç†çš„ä¿ç•™æœŸé™

```python
# æ ¹æ“šæ¥­å‹™éœ€æ±‚è¨­å®š
retention_rules = {
    "hot_news": 7,      # ç†±é–€æ–°èï¼š7 å¤©
    "regular_news": 30, # ä¸€èˆ¬æ–°èï¼š30 å¤©
    "archive": 90       # é‡è¦æ–°èï¼š90 å¤©æ­¸æª”
}
```

### 3. ç›£æ§èˆ‡å‘Šè­¦

```python
def monitor_storage():
    """ç›£æ§å„²å­˜ä½¿ç”¨æƒ…æ³"""

    stats = db.get_stats()

    if stats['total_size_gb'] > 5:  # è¶…é 5GB å‘Šè­¦
        send_alert(f"å„²å­˜ç©ºé–“éå¤§: {stats['total_size_gb']} GB")

    if stats['total_news'] > 500000:  # è¶…é 50 è¬æ¢å‘Šè­¦
        send_alert(f"æ–°èæ•¸é‡éå¤š: {stats['total_news']}")

# æ¯å°æ™‚æª¢æŸ¥
schedule.every().hour.do(monitor_storage)
```

### 4. å‚™ä»½ç­–ç•¥

```python
def backup_before_cleanup():
    """æ¸…ç†å‰å‚™ä»½"""

    cutoff = (datetime.now() - timedelta(days=30)).isoformat()

    # åŒ¯å‡ºå³å°‡åˆªé™¤çš„è³‡æ–™
    old_news = collection.get(
        where={"date": {"$lt": cutoff}}
    )

    # å„²å­˜åˆ° S3 æˆ–æœ¬åœ°
    with open(f"backup_{cutoff}.json", "w") as f:
        json.dump(old_news, f)

    # åŸ·è¡Œåˆªé™¤
    collection.delete(...)
```

### 5. æ¼¸é€²å¼åˆªé™¤

```python
def gradual_cleanup(batch_size: int = 1000):
    """åˆ†æ‰¹åˆªé™¤ï¼Œé¿å…å½±éŸ¿ç·šä¸Šæœå‹™"""

    cutoff = (datetime.now() - timedelta(days=30)).isoformat()

    while True:
        # æ¯æ¬¡åªåˆªé™¤ 1000 æ¢
        old_news = collection.get(
            where={"date": {"$lt": cutoff}},
            limit=batch_size
        )

        if not old_news['ids']:
            break

        collection.delete(ids=old_news['ids'])
        time.sleep(1)  # æš«åœ 1 ç§’ï¼Œæ¸›å°‘è² è¼‰
```

---

## ğŸ¯ ç¸½çµ

### æ ¸å¿ƒå•é¡Œç­”æ¡ˆ

1. **æ‰€æœ‰ä¸»æµå‘é‡è³‡æ–™åº«éƒ½æ”¯æ´åˆªé™¤å—ï¼Ÿ**
   - âœ… æ˜¯çš„ï¼Œéƒ½æ”¯æ´ç¡¬åˆªé™¤
   - â­ Qdrant å’Œ Elasticsearch é‚„æ”¯æ´ TTL/ILM

2. **å®šæ™‚åˆªé™¤èƒ½æ§åˆ¶å®¹é‡å—ï¼Ÿ**
   - âœ… å¯ä»¥ï¼Œæ˜¯æœ‰æ•ˆçš„å®¹é‡ç®¡ç†æ–¹æ¡ˆ
   - â­ æ¨è–¦ä½¿ç”¨ TTL è‡ªå‹•éæœŸï¼ˆæ›´å„ªï¼‰

### æ¨è–¦æ–¹æ¡ˆ

**æœ€ç°¡å–®**: ChromaDB + å®šæ™‚åˆªé™¤ (cron job)
**æœ€æ¨è–¦**: Qdrant + TTL è‡ªå‹•éæœŸ â­
**æœ€éˆæ´»**: Elasticsearch + ILM
**æœ€é«˜æ•ˆ**: æ»¾å‹•ç´¢å¼• + æ‰¹æ¬¡åˆªé™¤

### å®¹é‡ç®¡ç†é—œéµ

```
æ¯æ—¥æ–°è 10,000 ç¯‡:
â”œâ”€â”€ ä¿ç•™ 7 å¤© â†’ 273 MB âœ“ é©åˆå¿«é€Ÿæœç´¢
â”œâ”€â”€ ä¿ç•™ 30 å¤© â†’ 1.17 GB âœ“ å¹³è¡¡é»ï¼ˆæ¨è–¦ï¼‰
â”œâ”€â”€ ä¿ç•™ 90 å¤© â†’ 3.52 GB âš ï¸ éœ€è¦å„ªåŒ–
â””â”€â”€ ä¿ç•™ 1 å¹´ â†’ 14.26 GB âœ— ä¸å»ºè­°
```

### å¯¦ä½œå»ºè­°

1. **å¾ç°¡å–®é–‹å§‹**: ChromaDB + å®šæ™‚åˆªé™¤
2. **ç”Ÿç”¢ç’°å¢ƒ**: å‡ç´šåˆ° Qdrant + TTL
3. **ç›£æ§å®¹é‡**: è¨­å®šå‘Šè­¦é–¥å€¼
4. **å®šæœŸå‚™ä»½**: åˆªé™¤å‰åŒ¯å‡ºé‡è¦è³‡æ–™
5. **æ¸¬è©¦é©—è­‰**: åœ¨é–‹ç™¼ç’°å¢ƒå……åˆ†æ¸¬è©¦

---

**ç›¸é—œè³‡æº**:
- Qdrant æ–‡æª”: https://qdrant.tech/documentation/
- Elasticsearch ILM: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html
- ChromaDB æ–‡æª”: https://docs.trychroma.com/

**æ‚¨çš„å°ˆæ¡ˆ**:
- ç›®å‰ä½¿ç”¨: ChromaDB (`backend/src/services/memory_service.py`)
- å»ºè­°: å¯ä¿æŒç¾ç‹€æˆ–è€ƒæ…®å‡ç´šåˆ° Qdrant
