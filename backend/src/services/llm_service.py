"""
LLM æœå‹™æ¨¡çµ„ï¼šGoogle Gemini 2.5 Flash æ•´åˆ

æ­¤æ¨¡çµ„æä¾›å¤§å‹èªè¨€æ¨¡å‹çš„å°è©±åŠŸèƒ½ã€‚
"""

from typing import List, Optional

import google.generativeai as genai

from ..config import settings
from ..utils.logger import get_logger
from ..utils.exceptions import LLMError

logger = get_logger(__name__)


class LLMService:
    """LLM æœå‹™"""

    _model = None

    @classmethod
    def initialize(cls) -> None:
        """åˆå§‹åŒ– Google Gemini å®¢æˆ¶ç«¯"""
        try:
            genai.configure(api_key=settings.google_api_key)
            cls._model = genai.GenerativeModel(settings.mem0_llm_model)
            logger.info(f"Google Gemini å®¢æˆ¶ç«¯å·²åˆå§‹åŒ–: {settings.mem0_llm_model}")
        except Exception as e:
            logger.error(f"Google Gemini åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            raise LLMError(f"ç„¡æ³•åˆå§‹åŒ– LLM æœå‹™: {str(e)}")

    @classmethod
    def generate_response(
        cls,
        user_input: str,
        memories: Optional[List] = None,
        conversation_history: Optional[List[dict]] = None,
    ) -> str:
        """
        ç”Ÿæˆ LLM å›æ‡‰ï¼ˆUS2 T039 æ”¹é€²ï¼‰

        Args:
            user_input: ä½¿ç”¨è€…è¼¸å…¥
            memories: ç›¸é—œè¨˜æ†¶åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯å­—ä¸²æˆ–å­—å…¸åˆ—è¡¨ï¼‰
            conversation_history: å°è©±æ­·å²ï¼ˆé¸ç”¨ï¼‰

        Returns:
            str: LLM å›æ‡‰

        Raises:
            LLMError: å¦‚æœç”Ÿæˆå¤±æ•—
        """
        try:
            if cls._model is None:
                cls.initialize()

            # æ§‹å»ºç³»çµ±æç¤º - ä½¿ç”¨ç°¡æ½”ä¸­æ€§çš„æªè¾­
            system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­ã€å‹å–„çš„æŠ•è³‡é¡§å•åŠ©ç†ã€‚
è«‹æ ¹æ“šä½¿ç”¨è€…çš„éœ€æ±‚æä¾›è³‡è¨Šå’Œå»ºè­°ã€‚
ä½¿ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ï¼Œä¿æŒç°¡æ½”æ˜ç­ã€‚
"""

            # æ–°å¢è¨˜æ†¶ä¸Šä¸‹æ–‡ï¼ˆUS2 T039ï¼‰
            if memories and len(memories) > 0:
                logger.info(f"ğŸ§  æ³¨å…¥è¨˜æ†¶: å…± {len(memories)} å€‹")
                
                # æå–å¯¦éš›çš„è¨˜æ†¶å…§å®¹
                memory_contents = []
                for idx, memory in enumerate(memories):
                    # æ”¯æ´å­—å…¸æ ¼å¼ï¼ˆæ–°å¢ï¼‰æˆ–å­—ä¸²æ ¼å¼ï¼ˆèˆŠç‰ˆæœ¬ç›¸å®¹ï¼‰
                    if isinstance(memory, dict):
                        content = memory.get("content", "")
                    else:
                        content = str(memory)
                    
                    if content:
                        memory_contents.append(content)
                        logger.debug(f"  [{idx+1}] è¨˜æ†¶ ID: {memory.get('id', 'N/A') if isinstance(memory, dict) else 'N/A'}, Content: {content[:50]}")
                
                # åªæœ‰ç•¶æœ‰å¯¦éš›è¨˜æ†¶å…§å®¹æ™‚ï¼Œæ‰æ·»åŠ åˆ° system prompt
                if memory_contents:
                    memory_context = "å·²çŸ¥çš„ä½¿ç”¨è€…ä¿¡æ¯èˆ‡æŠ•è³‡åå¥½ï¼š\n"
                    for content in memory_contents:
                        memory_context += f"â€¢ {content}\n"
                    memory_context += "\nè«‹åŸºæ–¼ä¸Šè¿°ä½¿ç”¨è€…ä¿¡æ¯æä¾›å€‹äººåŒ–çš„æŠ•è³‡å»ºè­°ã€‚\n"
                    system_prompt += memory_context
                    logger.info(f"âœ“ è¨˜æ†¶å·²æˆåŠŸæ³¨å…¥åˆ° prompt ({len(memory_contents)} é …)")
                else:
                    logger.warning(f"âš ï¸ è¨˜æ†¶çµæœæœ‰ {len(memories)} å€‹ä½†å…§å®¹å…¨ç‚ºç©º")
            else:
                logger.info("â„¹ï¸ æœªæ‰¾åˆ°è¨˜æ†¶ (memories ç‚ºç©ºæˆ– None)")

            # æ§‹å»ºå°è©±æ­·å²ä¸Šä¸‹æ–‡
            history_context = ""
            if conversation_history:
                history_context = "\nå°è©±æ­·å²ï¼š\n"
                for msg in conversation_history:
                    role = msg.get("role", "unknown")
                    content = msg.get("content", "")
                    if role == "user":
                        history_context += f"ä½¿ç”¨è€…: {content}\n"
                    elif role == "assistant":
                        history_context += f"åŠ©ç†: {content}\n"
                
                system_prompt += history_context

            # æ§‹å»ºæç¤º
            full_prompt = f"""{system_prompt}

ã€å°è©±è¨˜éŒ„ã€‘
{history_context if history_context else "(é¦–æ¬¡å°è©±)"}

ã€ç•¶å‰æå•ã€‘
{user_input}

ã€è¦æ±‚ã€‘
- è«‹åŸºæ–¼å·²çŸ¥çš„ä½¿ç”¨è€…ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰ä¾†å€‹äººåŒ–å›æ‡‰
- é¿å…é‡è¤‡è©¢å•å·²çŸ¥çš„ä¿¡æ¯
- æä¾›å…·é«”çš„æŠ•è³‡å»ºè­°è€Œéæ³›æ³›è€Œè«‡
- å¦‚æœå°šç¼ºç›¸é—œä¿¡æ¯ï¼Œå¯è©¢å•ä½†è¦æŒ‡å‡ºå·²çŸ¥å…§å®¹

ã€å›æ‡‰ã€‘
"""

            # é…ç½®å®‰å…¨è¨­å®š - ä½¿ç”¨å¯¬é¬†çš„å®‰å…¨ç´šåˆ¥ä»¥æ”¯æ´é‡‘è/æŠ•è³‡å…§å®¹
            # BLOCK_ONLY_HIGH åªé˜»æ“‹æœ€åš´é‡çš„å…§å®¹
            safety_settings = [
                {
                    "category": genai.types.HarmCategory.HARM_CATEGORY_HARASSMENT,
                    "threshold": genai.types.HarmBlockThreshold.BLOCK_ONLY_HIGH,
                },
                {
                    "category": genai.types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                    "threshold": genai.types.HarmBlockThreshold.BLOCK_ONLY_HIGH,
                },
                {
                    "category": genai.types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                    "threshold": genai.types.HarmBlockThreshold.BLOCK_ONLY_HIGH,
                },
                {
                    "category": genai.types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                    "threshold": genai.types.HarmBlockThreshold.BLOCK_ONLY_HIGH,
                },
            ]

            logger.debug(f"ç™¼é€ LLM è«‹æ±‚ï¼Œprompt é•·åº¦: {len(full_prompt)} å­—å…ƒï¼Œè¨˜æ†¶æ•¸: {len(memories) if memories else 0}")

            # å‘¼å«æ¨¡å‹
            response = cls._model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=500,
                ),
                safety_settings=safety_settings,
            )

            logger.debug(f"LLM å›æ‡‰ç‹€æ…‹: finish_reason={getattr(response, 'finish_reason', 'unknown')}")

            # å–å¾— finish_reason
            finish_reason = getattr(response, 'finish_reason', None)
            finish_reason_name = finish_reason.name if finish_reason and hasattr(finish_reason, 'name') else str(finish_reason)
            
            logger.debug(f"finish_reason è©³æƒ…: {finish_reason} (name={finish_reason_name})")

            # æª¢æŸ¥ finish_reason ä»¥åˆ¤æ–·æ˜¯å¦å› ç‚ºå®‰å…¨åŸå› è¢«é˜»æ“‹
            if finish_reason and finish_reason_name == "SAFETY":
                logger.warning(
                    f"LLM å›æ‡‰å› å®‰å…¨åŸå› è¢«é˜»æ“‹ (finish_reason=SAFETY)ï¼Œä½¿ç”¨å‚™ç”¨å›æ‡‰"
                )
                # è¿”å›å‚™ç”¨å›æ‡‰è€Œä¸æ˜¯æ‹‹å‡ºç•°å¸¸
                return "æ„Ÿè¬æ‚¨çš„æå•ã€‚ç‚ºäº†æä¾›æ›´å¥½çš„æœå‹™ï¼Œè«‹ç”¨ä¸åŒçš„æ–¹å¼è¡¨é”æ‚¨çš„å•é¡Œã€‚"

            # æª¢æŸ¥æ˜¯å¦æœ‰ prompt_feedback ä¸­çš„é˜»æ“‹åŸå› 
            if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:
                logger.warning(
                    f"LLM å›æ‡‰è¢«å®‰å…¨éæ¿¾å™¨é˜»æ“‹ã€‚"
                    f"Block reason: {response.prompt_feedback.block_reason}ï¼Œä½¿ç”¨å‚™ç”¨å›æ‡‰"
                )
                # è¿”å›å‚™ç”¨å›æ‡‰è€Œä¸æ˜¯æ‹‹å‡ºç•°å¸¸
                return "æ„Ÿè¬æ‚¨çš„æå•ã€‚æˆ‘å€‘ç„¡æ³•è™•ç†æ­¤è«‹æ±‚ï¼Œè«‹ç¨å¾Œé‡è©¦æˆ–ä½¿ç”¨ä¸åŒçš„æ–¹å¼è¡¨é”ã€‚"

            # å®‰å…¨åœ°å–å¾—å›æ‡‰æ–‡æœ¬ï¼Œé¿å…è§¸ç™¼å¿«é€Ÿè¨ªå•å™¨ç•°å¸¸
            try:
                text = None
                if response and response.candidates and len(response.candidates) > 0:
                    candidate = response.candidates[0]
                    # æª¢æŸ¥æ˜¯å¦æœ‰å…§å®¹å’Œéƒ¨åˆ†
                    if candidate.content and hasattr(candidate.content, 'parts'):
                        parts = candidate.content.parts
                        if parts:  # æª¢æŸ¥ parts æ˜¯å¦ä¸ç‚ºç©º
                            text = "".join(part.text for part in parts if hasattr(part, 'text'))
                
                # å¦‚æœæˆåŠŸå–å¾—æ–‡æœ¬
                if text:
                    logger.info(
                        f"LLM å›æ‡‰æˆåŠŸ (tokens: {len(text.split())}, "
                        f"finish_reason: {finish_reason_name}, "
                        f"memories_used: {len(memories) if memories else 0})"
                    )
                    return text
                
                # å¦‚æœæ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å›æ‡‰éƒ¨åˆ†ï¼Œè¨˜éŒ„è©³ç´°ä¿¡æ¯ç”¨æ–¼èª¿è©¦
                has_candidates = response and response.candidates and len(response.candidates) > 0
                has_content = has_candidates and response.candidates[0].content is not None
                has_parts = has_content and hasattr(response.candidates[0].content, 'parts')
                parts_content = response.candidates[0].content.parts if has_parts else None
                parts_len = len(parts_content) if parts_content else 0
                
                # æª¢æŸ¥å®‰å…¨è©•ç´šå’Œå€™é¸è€…çš„é˜»æ“‹åŸå› 
                candidate_finish_reason = None
                safety_ratings = None
                if has_candidates:
                    candidate = response.candidates[0]
                    candidate_finish_reason = getattr(candidate, 'finish_reason', None)
                    safety_ratings = getattr(candidate, 'safety_ratings', None)
                
                logger.warning(
                    f"LLM å›æ‡‰ç‚ºç©º: "
                    f"finish_reason={finish_reason_name}, "
                    f"candidate_finish_reason={candidate_finish_reason}, "
                    f"has_candidates={has_candidates}, "
                    f"has_content={has_content}, "
                    f"has_parts={has_parts}, "
                    f"parts_len={parts_len}"
                )
                
                # è¨˜éŒ„å®‰å…¨è©•ç´šä»¥ä¾¿è¨ºæ–·
                if safety_ratings:
                    logger.warning(f"Safety ratings: {safety_ratings}")
                
                # å¦‚æœå€™é¸è€…çš„ finish_reason æ˜¯ SAFETY
                if candidate_finish_reason:
                    candidate_finish_reason_name = candidate_finish_reason.name if hasattr(candidate_finish_reason, 'name') else str(candidate_finish_reason)
                    if candidate_finish_reason_name == "SAFETY":
                        logger.warning("å€™é¸è€…å› å®‰å…¨åŸå› è¢«é˜»æ“‹ï¼Œä½¿ç”¨å‚™ç”¨å›æ‡‰")
                        return "æ„Ÿè¬æ‚¨çš„æå•ã€‚ç‚ºäº†æä¾›æ›´å¥½çš„æœå‹™ï¼Œè«‹ç”¨ä¸åŒçš„æ–¹å¼è¡¨é”æ‚¨çš„å•é¡Œã€‚"
                
                # å›æ‡‰ç‚ºç©ºï¼Œå¯èƒ½æ˜¯ç”±æ–¼å…§å®¹å¯©æ ¸æˆ–å…¶ä»–åŸå› ï¼Œè¿”å›å‚™ç”¨å›æ‡‰
                logger.warning("LLM å›æ‡‰ç‚ºç©ºï¼Œè¿”å›å‚™ç”¨å›æ‡‰")
                return "æ„Ÿè¬æ‚¨çš„æå•ã€‚è«‹ç¨å¾Œé‡è©¦ã€‚"
            except ValueError as e:
                # é€™é€šå¸¸æ˜¯ç”± response.text å¿«é€Ÿè¨ªå•å™¨æ‹‹å‡ºçš„
                logger.error(
                    f"LLM å›æ‡‰ç„¡æ•ˆ (ValueError): {str(e)}"
                )
                raise LLMError(f"LLM å›æ‡‰ç„¡æ•ˆ: {str(e)}")

        except Exception as e:
            logger.error(f"LLM ç”Ÿæˆå¤±æ•—: {str(e)}")
            raise LLMError(f"ç„¡æ³•ç”Ÿæˆå›æ‡‰: {str(e)}")

    @classmethod
    def extract_preferences(cls, text: str) -> Optional[str]:
        """
        å¾æ–‡æœ¬ä¸­æå–æŠ•è³‡åå¥½

        Args:
            text: ä½¿ç”¨è€…è¼¸å…¥

        Returns:
            Optional[str]: æå–çš„åå¥½æ–‡æœ¬

        Raises:
            LLMError: å¦‚æœæå–å¤±æ•—
        """
        try:
            if cls._model is None:
                cls.initialize()

            extraction_prompt = f"""åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œæå–ä»»ä½•æŠ•è³‡ç›¸é—œçš„åå¥½ã€ç›®æ¨™æˆ–é¢¨éšªåå¥½ã€‚
å¦‚æœæ²’æœ‰æŠ•è³‡ç›¸é—œä¿¡æ¯ï¼Œè¿”å› 'NONE'ã€‚
åªè¿”å›æå–çš„ä¿¡æ¯ï¼Œä¸è¦åŠ å…¥ä»»ä½•å…¶ä»–è§£é‡‹ã€‚

æ–‡æœ¬: {text}

æå–çš„åå¥½:"""

            # é…ç½®å®‰å…¨è¨­å®š - æš«æ™‚ä½¿ç”¨æœ€å¯¬é¬†çš„è¨­å®šé€²è¡Œæ¸¬è©¦
            safety_settings = [
                {
                    "category": genai.types.HarmCategory.HARM_CATEGORY_HARASSMENT,
                    "threshold": genai.types.HarmBlockThreshold.BLOCK_NONE,
                },
                {
                    "category": genai.types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                    "threshold": genai.types.HarmBlockThreshold.BLOCK_NONE,
                },
                {
                    "category": genai.types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                    "threshold": genai.types.HarmBlockThreshold.BLOCK_NONE,
                },
                {
                    "category": genai.types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                    "threshold": genai.types.HarmBlockThreshold.BLOCK_NONE,
                },
            ]

            response = cls._model.generate_content(
                extraction_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,
                    max_output_tokens=200,
                ),
                safety_settings=safety_settings,
            )

            # æª¢æŸ¥ finish_reason ä»¥åˆ¤æ–·æ˜¯å¦å› ç‚ºå®‰å…¨åŸå› è¢«é˜»æ“‹
            finish_reason = getattr(response, 'finish_reason', None)
            if finish_reason and finish_reason.name == "SAFETY":
                logger.warning(
                    f"åå¥½æå–å› å®‰å…¨åŸå› è¢«é˜»æ“‹ (finish_reason=SAFETY)"
                )
                return None

            # æª¢æŸ¥æ˜¯å¦æœ‰ prompt_feedback ä¸­çš„é˜»æ“‹åŸå› 
            if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:
                logger.warning(
                    f"åå¥½æå–è¢«å®‰å…¨éæ¿¾å™¨é˜»æ“‹: block_reason={response.prompt_feedback.block_reason}"
                )
                return None

            # å®‰å…¨åœ°å–å¾—å›æ‡‰æ–‡æœ¬
            try:
                if response and response.candidates and len(response.candidates) > 0:
                    candidate = response.candidates[0]
                    # æª¢æŸ¥æ˜¯å¦æœ‰å…§å®¹
                    if candidate.content and hasattr(candidate.content, 'parts'):
                        parts = candidate.content.parts
                        if parts and len(parts) > 0:
                            text = "".join(part.text for part in parts if hasattr(part, 'text'))
                            if text:
                                result = text.strip()
                                if result == "NONE":
                                    logger.debug("ç”¨æˆ¶æ¶ˆæ¯ä¸­æœªæ‰¾åˆ°æŠ•è³‡åå¥½")
                                    return None
                                logger.info(f"æˆåŠŸæå–æŠ•è³‡åå¥½: {result[:100]}")
                                return result
                
                # å¦‚æœæ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å›æ‡‰éƒ¨åˆ†
                logger.debug(
                    f"åå¥½æå–æœªè¿”å›æœ‰æ•ˆå›æ‡‰ï¼Œfinish_reason: {finish_reason}"
                )
                return None
            except ValueError as e:
                # é€™é€šå¸¸æ˜¯ç”±å¿«é€Ÿè¨ªå•å™¨æ‹‹å‡ºçš„
                logger.debug(f"åå¥½æå–å›æ‡‰ç„¡æ•ˆ: {str(e)}")
                return None

        except Exception as e:
            logger.error(f"åå¥½æå–å¤±æ•—: {str(e)}")
            return None
